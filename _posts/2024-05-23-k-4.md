---
title: "[Kubernetes]-도커&쿠버네티스 4일차"
excerpt: "쿠버네티스 업데이트 & 모니터링"

categories:
  - Kubernetes
tags:
  - [kubernetes, docker]

permalink: /kubernetes/k-4/

toc: true
toc_sticky: true

date: 2024-05-23
last_modified_at: 2024-05-23
---
# 쿠버네티스 4일차

### Kubernetes Architecture

![kube](https://github.com/yblmmen/gatsby.github.io/assets/161982180/39f86e39-0c15-432a-8295-077f0bb40148)
![kube2](https://github.com/yblmmen/gatsby.github.io/assets/161982180/b35ddec4-25a0-48b2-a5fb-8f388d8b3d21)



### 1. 네임스페이스

- namespace Archetecture
![namespace](https://github.com/yblmmen/gatsby.github.io/assets/161982180/af4adf56-5450-414f-a86d-aa6cfdde0062)

#### 네임스페이스의 기능
  - 자원 분리 및 접근제어 기능


#### kubectl 기본 명령어

```
# mkdir workspace && cd $_
# kubectl api-resources    // 버전,  shortnames 정보등을 볼수 있음
  - shortname -> 예) kubectl get no
  - pod(task) = container + volume + side car (side car: container 감시 기능)

# kubectl [명령어] [유형] [이름] [옵션]
# kubectl get node
# kubectl run nginx-pod --image=nginx # pending > ContainerCreating > Running
# kubectl get pod
# kubectl get service

// cluster ip 서비스 생성 (클러스터 내 노드와 POD 간 통신 가능한 서비스)
# kubectl expose pod nginx-pod --name clusterip --type=ClusterIP --port 80

// nodeport & Loadbalancer (외부에서 접속 가능하게 해줌), 노드포트와 cluser ip는 동시에 설정할 필요없음
// NodePort를 사용하면 포트당 하나의 서비스를 사용하며, 30000-32767 범위 내의 포트를 사용 가능하다.
# kubectl expose pod nginx-pod --name nodeport --type=NodePort --port 80     // type 설정은 반드시 pascal

// external ip : 노드들의 private ip
// GCP가 외부 IP를 Private-ip 로 NAT시켜줌
# kubectl expose pod nginx-pod --name loadbalancer --type=LoadBalancer --external-ip 10.178.0.7 --port 80
# kubectl get pod
# kubectl get service

// pod 내 bash 접속
# kubectl exec -it nginx-pod -- bash


# kubectl cp html nginx-pod:/usr/share/nginx
# kubectl get all

// 동일 네임스페이스 내에서 delete 가능
# kubectl delete svc --all
# kubectl delete pod nginx-pod
# kubectl delete pod,svc --all      // 주의사항: , 사이에 절대 스페이스 넣지 말것
# kubectl delete pod <pod> --grace-period=0 --force  // pod 강제삭제, but 자가 수복기능으로 새로 pod 가 생성됨  

// kubectl 옵션 정보
# kubectl options
# kubectl api-resources

# kubectl get nodes -o wide
# kubectl get nodes -o wide --no-headers

// 특정 필드번호{print $6}를 추출해서 출력 
# kubectl get nodes -o wide --no-headers | awk '{print $6}'

```


#### 노드포트 실습
  
  - 참고 [쿠버네티스 치트시트](https://kubernetes.io/ko/docs/reference/kubectl/cheatsheet/)  

```
// kube-proxy 가 접속 연결 수행

// 사전 실습환경 체크사항
# master1 : 34.47.77.56  /  private ip : 10.178.0.7
# worker1 : 34.47.75.9   /  private ip : 10.178.0.8
# worker2 : 34.47.89.46  /  private ip : 10.178.0.9

// 실습

# nodeport: 31718
http://34.47.77.56:31718/
http://34.47.75.9:31718/
http://34.47.89.46:31718/

# loadbalance : 30348
http://34.47.77.56:30348/
http://34.47.75.9:30348/
http://34.47.89.46:30348/

kubectl expose pod nginx-pod --name loadbalancer --type=LoadBalancer --external-ip 10.178.0.7 --port 80
```

### 2. 디플로이먼트

- 쿠버네티스의 가장 핵심 컴포넌트

#### 디플로이먼트 실습

```
// deployment > replicaset > pod  의 순으로 종속관계
// replicas : pod 갯수, nginx-app : deployment
# kubectl create deployment nginx-app --replicas=2 --image=nginx  
# kubectl get deployments.apps     // pod 관리 -> rolling 업데이트 O
# kubectl get replicasets.apps     // pod를 복제(=리플리카셋 컨트롤러) -> rolling 업데이트 X
# kubectl get pods
# kubectl expose deployment nginx-app --name clusterip-app --type=ClusterIP --port 80

// clusterip 로드밸런싱 테스트
kubectl cp nginx-app-d757cd54d-g9mbb:/var/www/html/index.html ./
kubectl cp index.html nginx-app-d757cd54d-g9mbb:/var/www/html/index.html
curl 10.102.201.117
curl 10.244.1.4
curl 10.244.2.4

// 로드밸런싱 구성정보 확인
kubectl describe svc clusterip-app
kubectl get all -o wide
kubectl get pod,svc -o wide

// node port
# kubectl expose deployment nginx-app --name nodeport-app --type=NodePort --port 80

//loadbalancer
# kubectl expose deployment nginx-app --name loadbalancer-app --type=LoadBalancer --external-ip 10.178.0.7 --port 80
# kubectl get service

// scale :  kube-scheluler 가 균형분산 기능 수행
# kubectl scale deployment nginx-app --replicas=4  // 스케일 아웃
# kubectl get pods
# kubectl scale deployment nginx-app --replicas=2  // 스케일 인
# kubectl get pods
# kubectl delete deployments.apps nginx-app
# kubectl delete service --all    // 주요 자원 삭제

```



#### 네임스페이스 실습

```
# kubectl get namespaces (= kubectl get ns)

// 특정 네임스페이스 확인
# kubectl get pod -n kube-system

// get-context : 계정과 클러스터 가져오는 명령어
# kubectl config get-contexts kubernetes-admin@kubernetes

// NAMESPACE 부분이 없으면 defaul 네임스페이스
 

// 기본 네임스페이스 변경
# kubectl config set-context kubernetes-admin@kubernetes --namespace=kube-system
# kubectl config set-context kubernetes-admin@kubernetes --namespace=default


// 디폴트로 복원
# kubectl config get-contexts kubernetes-admin@kubernetes
# kubectl config set-context kubernetes-admin@kubernetes --namespace=default


// 신규 namespace 생성
# kubectl create namespace test-namespace
# kubectl get namespace

// 테스트 네임스페이스로 변경
# kubectl config set-context kubernetes-admin@kubernetes --namespace=test-namespace
# kubectl config set-context kubernetes-admin@kubernetes --namespace=default

// 네임스페이스 삭제
kubectl delete ns test-namespace
kubectl get ns
```



#### 쿠버네티스 POD

```
# vi nginx-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx-pod
  labels:           // pod 생성시 네트워크 서비스 연결고리
    app: nginx-pod
spec:
  containers:
  - name: nginx-pod-container
    image: nginx

# kubectl apply -f nginx-pod.yaml
# kubectl get pod -o wide
# kubectl describe pod nginx-pod

# vi clusterip-pod.yaml
apiVersion: v1
kind: Service
metadata:
  name: clusterip-pod
spec:
  type: ClusterIP
  selector:
    app: nginx-pod
  ports:
  - protocol: TCP
    port: 80
    targetPort: 80

# kubectl apply -f nodeport-pod.yaml
# kubectl get svc -o wide
# kubectl describe svc nodeport-pod
# kubectl edit svc nodeport-pod

# vi loadbalancer-pod.yaml
apiVersion: v1
kind: Service
metadata:
  name: loadbalancer-pod
spec:
  type: LoadBalancer
  externalIPs:
  - 192.168.1.188
  - 192.168.1.211
  - 192.168.1.216
  selector:
    app: nginx-pod
  ports:
  - protocol: TCP
    port: 80
    targetPort: 80

# kubectl apply -f loadbalancer-pod.yaml
# kubectl get svc -o wide
# kubectl describe svc loadbalancer-pod
# kubectl edit svc loadbalancer-pod

```

#### 쿠버네티스 디플로이먼트 yaml

```
# vi deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  replicas: 2
  selector:
    matchLabels:
      app: nginx-deployment

  template:
    metadata:
      name: nginx-deployment
      labels:
        app: nginx-deployment
    spec:
      containers:
      - name: nginx-deployment-container
        image: nginx
        ports:
        - containerPort: 80

# kubectl apply -f deployment.yaml
# kubectl get deployments.apps -o wide
# kubectl describe deployments.apps nginx-deployment
# kubectl edit deployments.apps nginx-deployment

# vi clusterip-deployment.yaml # 클러스터아이피 야믈
apiVersion: v1
kind: Service
metadata:
  name: clusterip-deployment
spec:
  type: ClusterIP
  selector:
    app: nginx-deployment
  ports:
  - protocol: TCP
    port: 80
    targetPort: 80

# kubectl apply -f clusterip-deployment.yaml
# kubectl get svc -o wide
# kubectl describe svc clusterip-deployment
# kubectl edit svc clusterip-deployment

# vi nodeport-deployment.yaml # 노드포트 야믈
apiVersion: v1
kind: Service
metadata:
  name: nodeport-deployment
spec:
  type: NodePort
  selector:
    app: nginx-deployment
  ports:
  - protocol: TCP
    port: 80
    targetPort: 80
    nodePort: 30080

# kubectl apply -f nodeport-deployment.yaml
# kubectl get svc -o wide
# kubectl describe svc nodeport-deployment
# kubectl edit svc nodeport-deployment

# vi loadbalancer-deployment.yaml # 로드밸런서 야믈
apiVersion: v1
kind: Service
metadata:
  name: loadbalancer-deployment
spec:
  type: LoadBalancer
  externalIPs:
  - 192.168.0.143
  selector:
    app: nginx-deployment
  ports:
  - protocol: TCP
    port: 80
    targetPort: 80

# kubectl apply -f loadbalancer-deployment.yaml
# kubectl get svc -o wide
# kubectl describe svc loadbalancer-deployment
# kubectl edit svc loadbalancer-deployment

# kubectl get all
# kubectl delete pod,svc --all
# kubectl delete replicaset,svc --all
# kubectl delete deployment,svc --all
```

#### 쿠버네티스 컨피그맵, 시크릿 yaml

```
vi configmap-dev.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: config-dev
  namespace: default
data:
  DB_URL: localhost
  DB_USER: myuser
  DB_PASS: mypass
  DEBUG_INFO: debug

# kubectl apply -f configmap-dev.yaml
# kubectl describe configmaps config-dev

vi deployment-config01.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: configapp
  labels:
    app: configapp
spec:
  replicas: 1
  selector:
    matchLabels:
      app: configapp

  template:
    metadata:
      labels:
        app: configapp
    spec:
      containers:
      - name: testapp
        image: nginx
        ports:
        - containerPort: 8080
        env:
        - name: DEBUG_LEVEL
          valueFrom:
            configMapKeyRef:
              name: config-dev
              key: DEBUG_INFO
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: configapp
  name: configapp-svc
  namespace: default
spec:
  type: NodePort
  ports:
  - nodePort: 30800
    port: 8080
    protocol: TCP
    targetPort: 80
  selector:
    app: configapp

# kubectl apply -f deployment-config01.yaml
# kubectl exec -it configapp-64d4554b68-v55g5 -- bash

vi configmap-wordpress.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: config-wordpress
  namespace: default
data:
  MYSQL_ROOT_HOST: '%'
  MYSQL_DATABASE: wordpress

# kubectl apply -f configmap-wordpress.yaml
# kubectl describe configmaps config-wordpress

vi secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: my-secret
stringData:
  MYSQL_ROOT_PASSWORD: mode1752
  MYSQL_USER: wpuser
  MYSQL_PASSWORD: wppass

# kubectl apply -f secret.yaml
# kubectl describe secrets my-secret

# vi mysql-pod-svc.yaml
apiVersion: v1
kind: Pod
metadata:
  name: mysql-pod
  labels:
    app: mysql-pod
spec:
  containers:
  - name: mysql-container
    image: mysql:5.7
    envFrom:
    - configMapRef:
        name: config-wordpress
    - secretRef:
        name: my-secret
    ports:
    - containerPort: 3306
---
apiVersion: v1
kind: Service
metadata:
  name: mysql-svc
spec:
  type: ClusterIP
  selector:
    app: mysql-pod
  ports:
  - protocol: TCP
    port: 3306
    targetPort: 3306

# vi wordpress-pod-svc.yaml
apiVersion: v1
kind: Pod
metadata:
  name: wordpress-pod
  labels:
    app: wordpress-pod
spec:
  containers:
  - name: wordpress-container
    image: wordpress
    env:
    - name: WORDPRESS_DB_HOST
      value: mysql-svc
    - name: WORDPRESS_DB_USER
      valueFrom:
        secretKeyRef:
          name: my-secret
          key: MYSQL_USER
    - name: WORDPRESS_DB_PASSWORD
      valueFrom:
        secretKeyRef:
          name: my-secret
          key: MYSQL_PASSWORD
    - name: WORDPRESS_DB_NAME
      valueFrom:
        configMapKeyRef:
          name: config-wordpress
          key: MYSQL_DATABASE
    ports:
    - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: wordpress-svc
spec:
  type: LoadBalancer
  externalIPs:
  - 10.178.0.5
  selector:
    app: wordpress-pod
  ports:
  - protocol: TCP
    port: 80
    targetPort: 80

# kubectl apply -f wordpress-pod-svc.yaml

# vi mysql-deploy-svc.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mysql-deploy
  labels:
    app: mysql-deploy
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mysql-deploy
  template:
    metadata:
      labels:
        app: mysql-deploy
    spec:
      containers:
      - name: mysql-container
        image: mysql:5.7
        envFrom:
        - configMapRef:
            name: config-wordpress
        - secretRef:
            name: my-secret
        ports:
        - containerPort: 3306
---
apiVersion: v1
kind: Service
metadata:
  name: mysql-svc
spec:
  type: ClusterIP
  selector:
    app: mysql-deploy
  ports:
  - protocol: TCP
    port: 3306
    targetPort: 3306

# kubectl apply -f mysql-deploy-svc.yaml

# vi wordpress-deploy-svc.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: wordpress-deploy
  labels:
    app: wordpress-deploy
spec:
  replicas: 1
  selector:
    matchLabels:
      app: wordpress-deploy
  template:
    metadata:
      labels:
        app: wordpress-deploy
    spec:
      containers:
      - name: wordpress-container
        image: wordpress
        volumeMounts:
        - mountPath: /var/www/html
          name: hostpath-vol
        env:
        - name: WORDPRESS_DB_HOST
          value: mysql-svc
        - name: WORDPRESS_DB_USER
          valueFrom:
            secretKeyRef:
              name: my-secret
              key: MYSQL_USER
        - name: WORDPRESS_DB_PASSWORD
          valueFrom:
            secretKeyRef:
              name: my-secret
              key: MYSQL_PASSWORD
        - name: WORDPRESS_DB_NAME
          valueFrom:
            configMapKeyRef:
              name: config-wordpress
              key: MYSQL_DATABASE
        ports:
        - containerPort: 80
      volumes:
      - name: hostpath-vol
        hostPath:
          path: /tmp
          type: Directory
---
apiVersion: v1
kind: Service
metadata:
  name: wordpress-svc
spec:
  type: LoadBalancer
  externalIPs:
  - 10.178.0.6
  selector:
    app: wordpress-deploy
  ports:
  - protocol: TCP
    port: 80
    targetPort: 80
```

### 3. 쿠버네티스 볼륨

#### pv/pvc

```
# vi pv-pvc-pod.yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: task-pv-volume
  labels:
    type: local
spec:
  storageClassName: manual
  capacity:
    storage: 10Mi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/mnt/data"
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: task-pv-claim
spec:
  storageClassName: manual
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Mi
  selector:
    matchLabels:
      type: local
---
apiVersion: v1
kind: Pod
metadata:
  name: task-pv-pod
  labels:
    app: task-pv-pod
spec:
  volumes:
    - name: task-pv-storage
      persistentVolumeClaim:
        claimName: task-pv-claim
  containers:
    - name: task-pv-container
      image: nginx
      ports:
        - containerPort: 80
          name: "http-server"
      volumeMounts:
        - mountPath: "/usr/share/nginx/html"
          name: task-pv-storage

# kubectl apply -f pv-pvc-pod.yaml

* rm -rf /mnt/data
* kubectl apply -f .
* echo "HELLO" > /mnt/data/index.html
* kubectl delete pod task-pv-pod
* kubectl delete pvc task-pv-claim
* Retain 동일한 스토리지 자산을 재사용하려는 경우, 동일한 스토리지 자산 정의로 새 퍼시스턴트볼륨을 생성한다.
```

#### NFS

```
# yum install -y nfs-utils.x86_64
# mkdir /nfs_shared
# chmod 777 /nfs_shared/
# echo '/nfs_shared 10.178.0.0/16(rw,sync,no_root_squash)' >> /etc/exports
# systemctl enable --now nfs
# vi nfs-pv.yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: nfs-pv
spec:
  capacity:
    storage: 100Mi
  accessModes:
    - ReadWriteMany
  persistentVolumeReclaimPolicy: Retain
  nfs:
    server: 10.178.0.4
    path: /nfs_shared

# kubectl apply -f nfs-pv.yaml
# kubectl get pv
# vi nfs-pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: nfs-pvc
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 10Mi

# kubectl apply -f nfs-pvc.yaml
# kubectl get pvc
# kubectl get pv
# vi nfs-pvc-deploy.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nfs-pvc-deploy
spec:
  replicas: 4
  selector:
    matchLabels:
      app: nfs-pvc-deploy

  template:
    metadata:
      labels:
        app: nfs-pvc-deploy
    spec:
      containers:
      - name: nginx
        image: nginx
        volumeMounts:
        - name: nfs-vol
          mountPath: /usr/share/nginx/html
      volumes:
      - name: nfs-vol
        persistentVolumeClaim:
          claimName: nfs-pvc

# kubectl apply -f nfs-pvc-deploy.yaml
# kubectl get pod
# kubectl exec -it nfs-pvc-deploy-76bf944dd5-6j9gf -- /bin/bash
# kubectl expose deployment nfs-pvc-deploy --type=LoadBalancer --name=nfs-pvc-deploy-svc1 --external-ip 10.178.0.4 --port=80

```

### 4. 쿠버네티스 스테이트풀셋, 잡

```
# cd ~
# mkdir statefulset && cd $_
# vi pv-pvc-statefulset.yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-volume-1
spec:
  capacity:
    storage: 1Gi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: manual
  hostPath:
    path: /mnt/data-1
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-volume-2
spec:
  capacity:
    storage: 1Gi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: manual
  hostPath:
    path: /mnt/data-2
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-volume-3
spec:
  capacity:
    storage: 1Gi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: manual
  hostPath:
    path: /mnt/data-3
---
apiVersion: v1
kind: Service
metadata:
  name: web
spec:
  ports:
  - port: 80
    name: web
  clusterIP: None
  selector:
    app: nginx
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: web
spec:
  serviceName: "web"
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerPort: 80
          name: web
        volumeMounts:
        - name: data
          mountPath: /usr/share/nginx/html
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: "manual"
      resources:
        requests:
          storage: 1Gi
---
apiVersion: v1
kind: Service
metadata:
  name: lb-svc
spec:
  type: LoadBalancer
  externalIPs:
  - 10.178.0.6
  selector:
    app: nginx
  ports:
  - protocol: TCP
    port: 80
    targetPort: 80

# kubectl apply -f pv-pvc-statefulset.yaml
# kubectl exec -it web-0 -- bash
root@web-0:/# curl web-2.web.default.svc.cluster.local
# kubectl set image statefulsets/web nginx=halilinux/web-site:sale

- job
Kubernetes Job은 일회성 작업을 실행하기 위해 사용됩니다. Job은 작업이 완료될 때까지 한 번 또는 여러 번의 시도를 통해 컨테이너를 실행합니다.
# cd ~
# mkdir job_cronjob && cd $_
# vi job-test.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: hello-job
spec:
  template:
    spec:
      containers:
      - name: hello
        image: busybox
        command: ["echo", "Hello, World!"]
      restartPolicy: Never
  backoffLimit: 4
```

#### 쿠버네티스 크론잡

- Kubernetes CronJob은 정기적으로 반복되는 작업을 예약하기 위해 사용됩니다. CronJob은 지정된 스케줄에 따라 Job을 생성하고 실행합니다.

```
# vi cronjob-test.yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: hello-cronjob
spec:
  schedule: "*/1 * * * *"
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: hello
            image: busybox
            command: ["echo", "Hello, World!"]
          restartPolicy: Never
```

### 5. 디플로이먼트 버전 관리

#### Deployment 롤링 업데이트 제어

```
# kubectl set image deployment.apps/nginx-deployment nginx-deployment-container=halilinux/web-site:v1.0
# kubectl get all
# kubectl rollout history deployment nginx-deployment
# kubectl rollout history deployment nginx-deployment --revision=2 # 리비전2 상세보기
# kubectl rollout undo deployment nginx-deployment # 롤백(전 단계로 복원)
# kubectl get all
# kubectl rollout history deployment nginx-deployment
# kubectl rollout history deployment nginx-deployment --revision=3 # 리비전3 상세보기
# kubectl rollout undo deployment nginx-deployment --to-revision=2 # 리비전2로 복원
```

### 6. 데몬셋

> Kubernetes DaemonSet은 클러스터의 모든 (또는 선택된) 노드에서 단일 복사본의 Pod를 실행하도록 보장하는 리소스입니다. 이는 로그 수집, 모니터링 에이전트, 시스템 레벨의 데몬 등을 배포할 때 유용합니다. DaemonSet은 노드가 추가되거나 제거될 때 자동으로 Pod를 추가하거나 제거합니다.

```
# cd ~
# cd ~
# mkdir daemonset && cd $_
# cat <<EOF > ssd-monitor-daemonset.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: ssd-monitor
spec:
  selector:
    matchLabels:
      app: ssd-monitor
  template:
    metadata:
      labels:
        app: ssd-monitor
    spec:
      nodeSelector:
        disk: ssd
      containers:
      - name: main
        image: luksa/ssd-monitor
EOF

# kubectl apply -f ssd-monitor-daemonset.yaml
# kubectl get ds
# kubectl get po -o wide
# kubectl get nodes --show-labels
# kubectl label nodes worker1 worker2 disk=ssd
# kubectl get ds
# kubectl get po
# kubectl logs pods/ssd-monitor-6bhtv
```

### 7. 헬름

#### helm 설치 및 기본 명령어

```
# git clone https://github.com/johnlee0405/kiamol.git
# curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3
# chmod 700 get_helm.sh
# ./get_helm.sh
# echo 'source <(helm completion bash)' >>~/.bashrc
# helm repo add kiamol https://kiamol.net
# helm repo update
# helm search repo vweb --versions
# helm show values kiamol/vweb --version 1.0.0
# helm install --set servicePort=8010 --set replicaCount=2 ch10-vweb kiamol/vweb --version 1.0.0
# helm ls
# kubectl describe deployments.apps ch10-vweb
# kubectl get deployments.apps -l app.kubernetes.io/instance=ch10-vweb
# helm upgrade --set servicePort=8010 --set replicaCount=4 ch10-vweb kiamol/vweb --version 1.0.0
# kubectl get rs
# kubectl get svc
# kubectl edit svc ch10-vweb
spec:
  externalIPs:
  - 10.178.0.5
```

### 8. 쿠버네티스 플루언트디, 일래스틱서치

#### fluentd
```
# ls /var/log/containers/
# kubectl apply -f timecheck/
# kubectl logs -l app=timecheck --all-containers -n kiamol-ch13-dev --tail 1
# kubectl logs -l app=timecheck --all-containers -n kiamol-ch13-test --tail 1
# kubectl apply -f sleep.yaml
# kubectl exec -it deployments/sleep -- sh
cd /var/log/containers/
cat *-test_logger-*
exit

# kubectl get pod -o wide

# kubectl apply -f fluentbit/
# kubectl get po -n kiamol-ch13-logging -o wide
# kubectl logs -l app=fluent-bit -n kiamol-ch13-logging --tail 2
# kubectl get daemonsets.apps  -n kiamol-ch13-logging

# kubectl apply -f fluentbit/update/fluentbit-config-match.yaml
# kubectl rollout restart ds/fluent-bit -n kiamol-ch13-logging
# kubectl logs -l app=fluent-bit -n kiamol-ch13-logging --tail 1
```

#### elasticsearch & kibana

```
# kubectl apply -f elasticsearch
# kubectl get pod -l app=elasticsearch -n kiamol-ch13-logging
# kubectl patch svc kibana -p '{"spec": {"externalIPs": ["172.16.0.150"]}}' -n kiamol-ch13-logging
# kubectl apply -f fluentbit/update/fluentbit-config-elasticsearch.yaml
# kubectl rollout restart ds/fluent-bit -n kiamol-ch13-logging
# kubectl apply -f numbers/
# kubectl patch svc numbers-api-proxy -p '{"spec": {"externalIPs": ["172.16.0.150"]}}' -n kiamol-ch13-test
# curl http://172.16.0.150:8080/rng

kubernetes.labels.app:numbers-api AND log:dc4e0c87-b7bb-4f28-81d8-94726fdf4232

t log	   	2024-05-14T23:25:23.070311741+09:00 stdout F <4>Numbers.Api.Controllers.RngController[0] Unhealthy! Failure ID: dc4e0c87-b7bb-4f28-81d8-94726fdf4232
```

### 9. 프로메테우스, 그라파나

```
# helm repo add stable https://charts.helm.sh/stable
# helm search repo
# helm repo update
# helm install stable/mysql --generate-name
# helm list
# helm uninstall mysql-1672897024
# helm list
# helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
# helm search repo prometheus
# helm install prometheus-community/kube-prometheus-stack --generate-name
# kubectl edit service/kube-prometheus-stack-1672897171-grafana # NodePort로 변경
# kubectl get secret/kube-prometheus-stack-1688083950-grafana -o jsonpath="{.data.admin-password}" | base64 --decode;echo
admin/prom-operator
#  kubectl edit svc kube-prometheus-stack-1688-prometheus # NodePort로 변경
```






















